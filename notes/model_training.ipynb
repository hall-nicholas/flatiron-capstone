{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import functions as fn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7936 files belonging to 2 classes.\n",
      "Using 6349 files for training.\n",
      "Found 7936 files belonging to 2 classes.\n",
      "Using 1587 files for validation.\n",
      "Found 1974 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    '../created_data/train', \n",
    "    labels='inferred',\n",
    "    subset=\"training\",\n",
    "    validation_split=.2,\n",
    "    seed=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "val_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    '../created_data/train', \n",
    "    labels='inferred',\n",
    "    subset=\"validation\",\n",
    "    validation_split=.2,\n",
    "    seed=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "test_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    '../created_data/test', \n",
    "    labels='inferred',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for images, labels in train_data.take(1):\n",
    "#    for i in range(3):\n",
    "#        fn.display_image(images[i].numpy().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' plt.figure(figsize=(10, 10))\\nfor images, labels in train_data.take(1):\\n    for i in range(9):\\n        ax = plt.subplot(3, 3, i + 1)\\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\\n        plt.title(train_data.class_names[labels[i]])\\n        plt.axis(\"off\") '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_data.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_data.class_names[labels[i]])\n",
    "        plt.axis(\"off\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    #image =  image.astype(np.float32) / 255.\n",
    "    resized_image = tf.image.resize(image, [512,512])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess).prefetch(1)\n",
    "val_data = val_data.map(preprocess).prefetch(1)\n",
    "test_data = test_data.map(preprocess).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_1084\\3638164968.py:20: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  results = model_Xception.fit_generator(train_data,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.5142 - recall_m: 0.7058 - precision_m: 0.5183 - f1_m: 0.5685\n",
      "Epoch 1: saving model to ../models\\model_Xception_weights00000001.h5\n",
      "50/50 [==============================] - 297s 6s/step - loss: 0.6980 - accuracy: 0.5142 - recall_m: 0.7058 - precision_m: 0.5183 - f1_m: 0.5685 - val_loss: 0.6828 - val_accuracy: 0.5545 - val_recall_m: 0.3257 - val_precision_m: 0.6040 - val_f1_m: 0.4130\n",
      "Epoch 2/4\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6813 - accuracy: 0.5608 - recall_m: 0.5898 - precision_m: 0.5814 - f1_m: 0.5336\n",
      "Epoch 2: saving model to ../models\\model_Xception_weights00000002.h5\n",
      "50/50 [==============================] - 289s 6s/step - loss: 0.6813 - accuracy: 0.5608 - recall_m: 0.5898 - precision_m: 0.5814 - f1_m: 0.5336 - val_loss: 0.6681 - val_accuracy: 0.6144 - val_recall_m: 0.5995 - val_precision_m: 0.6223 - val_f1_m: 0.6043\n",
      "Epoch 2: early stopping\n"
     ]
    }
   ],
   "source": [
    "base_model_Xception = keras.applications.xception.Xception(weights = 'imagenet', include_top = False)\n",
    "\n",
    "model_dir = '../models'\n",
    "model_uuid = 'model_Xception'\n",
    "\n",
    "for layer in base_model_Xception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model_Xception.output)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(avg)\n",
    "model_Xception = keras.Model(inputs = base_model_Xception.input, outputs = output)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=0, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 0.2, momentum = 0.9, decay = 0.01)\n",
    "model_Xception.compile(loss = 'binary_crossentropy', optimizer = 'adam',  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "results = model_Xception.fit_generator(train_data,\n",
    "    epochs=4,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 178s 3s/step - loss: 0.6878 - accuracy: 0.5355 - recall_m: 0.2531 - precision_m: 0.4966 - f1_m: 0.3325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6878007054328918,\n",
       " 0.5354610085487366,\n",
       " 0.25312501192092896,\n",
       " 0.49660441279411316,\n",
       " 0.3324719965457916]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Xception.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_1084\\4262946862.py:21: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  results = model_Xception.fit_generator(train_data,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 0.6714 - accuracy: 0.5892 - recall_m: 0.6364 - precision_m: 0.5991 - f1_m: 0.5842\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "50/50 [==============================] - 290s 6s/step - loss: 0.6714 - accuracy: 0.5892 - recall_m: 0.6364 - precision_m: 0.5991 - f1_m: 0.5842 - val_loss: 0.6587 - val_accuracy: 0.6270 - val_recall_m: 0.7774 - val_precision_m: 0.6001 - val_f1_m: 0.6706\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.6062 - recall_m: 0.6909 - precision_m: 0.6164 - f1_m: 0.6229\n",
      "Epoch 2: saving model to ../models\\model_Xception2_weights00000002.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.6632 - accuracy: 0.6062 - recall_m: 0.6909 - precision_m: 0.6164 - f1_m: 0.6229 - val_loss: 0.6522 - val_accuracy: 0.6200 - val_recall_m: 0.8439 - val_precision_m: 0.5836 - val_f1_m: 0.6839\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6524 - accuracy: 0.6314 - recall_m: 0.6428 - precision_m: 0.6434 - f1_m: 0.6220\n",
      "Epoch 3: saving model to ../models\\model_Xception2_weights00000003.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.6524 - accuracy: 0.6314 - recall_m: 0.6428 - precision_m: 0.6434 - f1_m: 0.6220 - val_loss: 0.6424 - val_accuracy: 0.6610 - val_recall_m: 0.7628 - val_precision_m: 0.6332 - val_f1_m: 0.6869\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.6572 - recall_m: 0.6922 - precision_m: 0.6636 - f1_m: 0.6626\n",
      "Epoch 4: saving model to ../models\\model_Xception2_weights00000004.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.6436 - accuracy: 0.6572 - recall_m: 0.6922 - precision_m: 0.6636 - f1_m: 0.6626 - val_loss: 0.6367 - val_accuracy: 0.6597 - val_recall_m: 0.8100 - val_precision_m: 0.6266 - val_f1_m: 0.7012\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6398 - accuracy: 0.6484 - recall_m: 0.7202 - precision_m: 0.6417 - f1_m: 0.6648\n",
      "Epoch 5: saving model to ../models\\model_Xception2_weights00000005.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.6398 - accuracy: 0.6484 - recall_m: 0.7202 - precision_m: 0.6417 - f1_m: 0.6648 - val_loss: 0.6296 - val_accuracy: 0.6692 - val_recall_m: 0.7562 - val_precision_m: 0.6470 - val_f1_m: 0.6912\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.6547 - recall_m: 0.6769 - precision_m: 0.6687 - f1_m: 0.6449\n",
      "Epoch 6: saving model to ../models\\model_Xception2_weights00000006.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.6379 - accuracy: 0.6547 - recall_m: 0.6769 - precision_m: 0.6687 - f1_m: 0.6449 - val_loss: 0.6237 - val_accuracy: 0.6830 - val_recall_m: 0.7681 - val_precision_m: 0.6647 - val_f1_m: 0.7042\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6262 - accuracy: 0.6749 - recall_m: 0.6617 - precision_m: 0.6850 - f1_m: 0.6590\n",
      "Epoch 7: saving model to ../models\\model_Xception2_weights00000007.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.6262 - accuracy: 0.6749 - recall_m: 0.6617 - precision_m: 0.6850 - f1_m: 0.6590 - val_loss: 0.6189 - val_accuracy: 0.6894 - val_recall_m: 0.6406 - val_precision_m: 0.7167 - val_f1_m: 0.6668\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.6843 - recall_m: 0.6882 - precision_m: 0.6923 - f1_m: 0.6786\n",
      "Epoch 8: saving model to ../models\\model_Xception2_weights00000008.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.6216 - accuracy: 0.6843 - recall_m: 0.6882 - precision_m: 0.6923 - f1_m: 0.6786 - val_loss: 0.6140 - val_accuracy: 0.6818 - val_recall_m: 0.7845 - val_precision_m: 0.6560 - val_f1_m: 0.7086\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.6849 - recall_m: 0.6898 - precision_m: 0.6947 - f1_m: 0.6815\n",
      "Epoch 9: saving model to ../models\\model_Xception2_weights00000009.h5\n",
      "50/50 [==============================] - 286s 6s/step - loss: 0.6157 - accuracy: 0.6849 - recall_m: 0.6898 - precision_m: 0.6947 - f1_m: 0.6815 - val_loss: 0.6103 - val_accuracy: 0.6887 - val_recall_m: 0.8134 - val_precision_m: 0.6539 - val_f1_m: 0.7186\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6113 - accuracy: 0.6944 - recall_m: 0.7428 - precision_m: 0.6844 - f1_m: 0.7048\n",
      "Epoch 10: saving model to ../models\\model_Xception2_weights00000010.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.6113 - accuracy: 0.6944 - recall_m: 0.7428 - precision_m: 0.6844 - f1_m: 0.7048 - val_loss: 0.6036 - val_accuracy: 0.7070 - val_recall_m: 0.7015 - val_precision_m: 0.7155 - val_f1_m: 0.7027\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.7045 - recall_m: 0.7021 - precision_m: 0.7183 - f1_m: 0.7007\n",
      "Epoch 11: saving model to ../models\\model_Xception2_weights00000011.h5\n",
      "50/50 [==============================] - 286s 6s/step - loss: 0.6075 - accuracy: 0.7045 - recall_m: 0.7021 - precision_m: 0.7183 - f1_m: 0.7007 - val_loss: 0.5991 - val_accuracy: 0.7139 - val_recall_m: 0.7254 - val_precision_m: 0.7114 - val_f1_m: 0.7137\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.6950 - recall_m: 0.6889 - precision_m: 0.7022 - f1_m: 0.6875\n",
      "Epoch 12: saving model to ../models\\model_Xception2_weights00000012.h5\n",
      "50/50 [==============================] - 285s 6s/step - loss: 0.6018 - accuracy: 0.6950 - recall_m: 0.6889 - precision_m: 0.7022 - f1_m: 0.6875 - val_loss: 0.6000 - val_accuracy: 0.6975 - val_recall_m: 0.8531 - val_precision_m: 0.6538 - val_f1_m: 0.7364\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.7026 - recall_m: 0.7196 - precision_m: 0.7032 - f1_m: 0.7007\n",
      "Epoch 13: saving model to ../models\\model_Xception2_weights00000013.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.5993 - accuracy: 0.7026 - recall_m: 0.7196 - precision_m: 0.7032 - f1_m: 0.7007 - val_loss: 0.5935 - val_accuracy: 0.7026 - val_recall_m: 0.8138 - val_precision_m: 0.6681 - val_f1_m: 0.7271\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.7051 - recall_m: 0.7264 - precision_m: 0.7096 - f1_m: 0.7078\n",
      "Epoch 14: saving model to ../models\\model_Xception2_weights00000014.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.5952 - accuracy: 0.7051 - recall_m: 0.7264 - precision_m: 0.7096 - f1_m: 0.7078 - val_loss: 0.5870 - val_accuracy: 0.7227 - val_recall_m: 0.7377 - val_precision_m: 0.7184 - val_f1_m: 0.7220\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.7146 - recall_m: 0.6996 - precision_m: 0.7266 - f1_m: 0.7014\n",
      "Epoch 15: saving model to ../models\\model_Xception2_weights00000015.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.5907 - accuracy: 0.7146 - recall_m: 0.6996 - precision_m: 0.7266 - f1_m: 0.7014 - val_loss: 0.5851 - val_accuracy: 0.7158 - val_recall_m: 0.8097 - val_precision_m: 0.6823 - val_f1_m: 0.7333\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.7101 - recall_m: 0.7233 - precision_m: 0.7193 - f1_m: 0.7077\n",
      "Epoch 16: saving model to ../models\\model_Xception2_weights00000016.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.5875 - accuracy: 0.7101 - recall_m: 0.7233 - precision_m: 0.7193 - f1_m: 0.7077 - val_loss: 0.5831 - val_accuracy: 0.7114 - val_recall_m: 0.8296 - val_precision_m: 0.6761 - val_f1_m: 0.7402\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5830 - accuracy: 0.7196 - recall_m: 0.7369 - precision_m: 0.7264 - f1_m: 0.7228\n",
      "Epoch 17: saving model to ../models\\model_Xception2_weights00000017.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.5830 - accuracy: 0.7196 - recall_m: 0.7369 - precision_m: 0.7264 - f1_m: 0.7228 - val_loss: 0.5768 - val_accuracy: 0.7391 - val_recall_m: 0.6944 - val_precision_m: 0.7726 - val_f1_m: 0.7236\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5792 - accuracy: 0.7309 - recall_m: 0.7436 - precision_m: 0.7346 - f1_m: 0.7308\n",
      "Epoch 18: saving model to ../models\\model_Xception2_weights00000018.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.5792 - accuracy: 0.7309 - recall_m: 0.7436 - precision_m: 0.7346 - f1_m: 0.7308 - val_loss: 0.5742 - val_accuracy: 0.7297 - val_recall_m: 0.8159 - val_precision_m: 0.6997 - val_f1_m: 0.7465\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.7164 - recall_m: 0.7484 - precision_m: 0.7284 - f1_m: 0.7197\n",
      "Epoch 19: saving model to ../models\\model_Xception2_weights00000019.h5\n",
      "50/50 [==============================] - 288s 6s/step - loss: 0.5805 - accuracy: 0.7164 - recall_m: 0.7484 - precision_m: 0.7284 - f1_m: 0.7197 - val_loss: 0.5702 - val_accuracy: 0.7322 - val_recall_m: 0.7958 - val_precision_m: 0.7138 - val_f1_m: 0.7489\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.7316 - recall_m: 0.7136 - precision_m: 0.7429 - f1_m: 0.7193\n",
      "Epoch 20: saving model to ../models\\model_Xception2_weights00000020.h5\n",
      "50/50 [==============================] - 287s 6s/step - loss: 0.5737 - accuracy: 0.7316 - recall_m: 0.7136 - precision_m: 0.7429 - f1_m: 0.7193 - val_loss: 0.5666 - val_accuracy: 0.7360 - val_recall_m: 0.7877 - val_precision_m: 0.7162 - val_f1_m: 0.7471\n"
     ]
    }
   ],
   "source": [
    "base_model_Xception = keras.applications.xception.Xception(weights = 'imagenet', include_top = False)\n",
    "\n",
    "model_dir = '../models'\n",
    "model_uuid = 'model_Xception2'\n",
    "\n",
    "for layer in base_model_Xception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model_Xception.output)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(avg)\n",
    "model_Xception = keras.Model(inputs = base_model_Xception.input, outputs = output)\n",
    "\n",
    "model_Xception.load_weights(f'{model_dir}/model_Xception_weights00000002.h5')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=3, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "model_Xception.compile(loss = 'binary_crossentropy', optimizer = 'adam',  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "results = model_Xception.fit_generator(train_data,\n",
    "    epochs=20,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 178s 3s/step - loss: 0.7005 - accuracy: 0.5441 - recall_m: 0.3175 - precision_m: 0.4964 - f1_m: 0.3852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7004950642585754,\n",
       " 0.5440729260444641,\n",
       " 0.3174999952316284,\n",
       " 0.49641579389572144,\n",
       " 0.38519051671028137]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Xception.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_724\\2845532247.py:21: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  results = model_Xception.fit_generator(train_data,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 0.7067 - accuracy: 0.5098 - recall_m: 0.0654 - precision_m: 0.0621 - f1_m: 0.0628 \n",
      "Epoch 1: saving model to ../models\\model_Xception3_weights00000001.h5\n",
      "50/50 [==============================] - 950s 19s/step - loss: 0.7067 - accuracy: 0.5098 - recall_m: 0.0654 - precision_m: 0.0621 - f1_m: 0.0628 - val_loss: 0.6934 - val_accuracy: 0.4965 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/5\n",
      "34/50 [===================>..........] - ETA: 3:22 - loss: 0.6937 - accuracy: 0.4917 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 - f1_m: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "base_model_Xception = keras.applications.xception.Xception(weights = 'imagenet', include_top = False)\n",
    "\n",
    "model_dir = '../models'\n",
    "model_uuid = 'model_Xception3'\n",
    "\n",
    "for layer in base_model_Xception.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model_Xception.output)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(avg)\n",
    "model_Xception = keras.Model(inputs = base_model_Xception.input, outputs = output)\n",
    "\n",
    "model_Xception.load_weights(f'{model_dir}/model_Xception2_weights00000020.h5')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=3, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "model_Xception.compile(loss = 'binary_crossentropy', optimizer = 'adam',  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "results = model_Xception.fit_generator(train_data,\n",
    "    epochs=5,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_1084\\362580371.py:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  results = model_Xception.fit_generator(train_data,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 1/50 [..............................] - ETA: 4:17 - loss: 0.6846 - accuracy: 0.5625 - recall_m: 0.5263 - precision_m: 0.6667 - f1_m: 0.5882\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 2/50 [>.............................] - ETA: 2:27 - loss: 0.6831 - accuracy: 0.5156 - recall_m: 0.5263 - precision_m: 0.6111 - f1_m: 0.5644\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 3/50 [>.............................] - ETA: 2:25 - loss: 0.6859 - accuracy: 0.5208 - recall_m: 0.6398 - precision_m: 0.5741 - f1_m: 0.5876\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 4/50 [=>............................] - ETA: 2:22 - loss: 0.6811 - accuracy: 0.5391 - recall_m: 0.7298 - precision_m: 0.5645 - f1_m: 0.6151\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 5/50 [==>...........................] - ETA: 2:19 - loss: 0.6795 - accuracy: 0.5562 - recall_m: 0.7839 - precision_m: 0.5593 - f1_m: 0.6321\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 6/50 [==>...........................] - ETA: 2:15 - loss: 0.6892 - accuracy: 0.5104 - recall_m: 0.7604 - precision_m: 0.5216 - f1_m: 0.5999\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 7/50 [===>..........................] - ETA: 2:13 - loss: 0.6853 - accuracy: 0.5223 - recall_m: 0.7836 - precision_m: 0.5185 - f1_m: 0.6069\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 8/50 [===>..........................] - ETA: 2:09 - loss: 0.6845 - accuracy: 0.5430 - recall_m: 0.7868 - precision_m: 0.5461 - f1_m: 0.6276\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      " 9/50 [====>.........................] - ETA: 2:06 - loss: 0.6826 - accuracy: 0.5556 - recall_m: 0.7696 - precision_m: 0.5688 - f1_m: 0.6341\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "10/50 [=====>........................] - ETA: 2:03 - loss: 0.6819 - accuracy: 0.5656 - recall_m: 0.7542 - precision_m: 0.5690 - f1_m: 0.6299\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "11/50 [=====>........................] - ETA: 2:00 - loss: 0.6801 - accuracy: 0.5739 - recall_m: 0.7412 - precision_m: 0.5840 - f1_m: 0.6333\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "12/50 [======>.......................] - ETA: 1:57 - loss: 0.6797 - accuracy: 0.5781 - recall_m: 0.7211 - precision_m: 0.5909 - f1_m: 0.6281\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "13/50 [======>.......................] - ETA: 1:55 - loss: 0.6790 - accuracy: 0.5817 - recall_m: 0.7137 - precision_m: 0.5935 - f1_m: 0.6279\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "14/50 [=======>......................] - ETA: 1:52 - loss: 0.6782 - accuracy: 0.5826 - recall_m: 0.7047 - precision_m: 0.5957 - f1_m: 0.6263\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "15/50 [========>.....................] - ETA: 1:49 - loss: 0.6755 - accuracy: 0.5917 - recall_m: 0.7077 - precision_m: 0.6031 - f1_m: 0.6330\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "16/50 [========>.....................] - ETA: 1:46 - loss: 0.6735 - accuracy: 0.6055 - recall_m: 0.7113 - precision_m: 0.6196 - f1_m: 0.6443\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "17/50 [=========>....................] - ETA: 1:43 - loss: 0.6725 - accuracy: 0.6103 - recall_m: 0.7099 - precision_m: 0.6235 - f1_m: 0.6468\n",
      "Epoch 1: saving model to ../models\\model_Xception2_weights00000001.h5\n",
      "18/50 [=========>....................] - ETA: 1:40 - loss: 0.6741 - accuracy: 0.6007 - recall_m: 0.7022 - precision_m: 0.6111 - f1_m: 0.6370"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m csv_logger \u001b[39m=\u001b[39m CSVLogger(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmodel_uuid\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m, separator \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, append \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m model_Xception\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,  metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, recall_m, precision_m, f1_m])\n\u001b[1;32m----> 9\u001b[0m results \u001b[39m=\u001b[39m model_Xception\u001b[39m.\u001b[39;49mfit_generator(train_data,\n\u001b[0;32m     10\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_data,\n\u001b[0;32m     12\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping, model_checkpoint, csv_logger])\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\keras\\engine\\training.py:2209\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2199\u001b[0m \n\u001b[0;32m   2200\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[0;32m   2202\u001b[0m \u001b[39m  this endpoint.\u001b[39;00m\n\u001b[0;32m   2203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2204\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2205\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   2208\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m-> 2209\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2210\u001b[0m     generator,\n\u001b[0;32m   2211\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2212\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2213\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2214\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2215\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2216\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2217\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2218\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2219\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2220\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2221\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2222\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2223\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=3, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "model_Xception.compile(loss = 'binary_crossentropy', optimizer = 'adam',  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "results = model_Xception.fit_generator(train_data,\n",
    "    epochs=20,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "acc = results.history['accuracy']\n",
    "val_acc = results.history['val_accuracy']\n",
    "\n",
    "loss = results.history['loss']\n",
    "val_loss = results.history['val_loss']\n",
    "\n",
    "recall = results.history['recall_m']\n",
    "val_recall = results.history['val_recall_m']\n",
    "\n",
    "precision = results.history['precision_m']\n",
    "val_precision = results.history['val_precision_m']\n",
    "\n",
    "f1 = results.history['f1_m']\n",
    "val_f1 = results.history['val_f1_m']\n",
    "\n",
    "epochs_range = range(len(results.history['accuracy']))\n",
    "\n",
    "plt.figure(figsize=(22, 18))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(epochs_range, recall, label='Training Recall')\n",
    "plt.plot(epochs_range, val_recall, label='Validation Recall')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(epochs_range, precision, label='Training Precision')\n",
    "plt.plot(epochs_range, val_precision, label='Validation Precision')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(epochs_range, f1, label='Training F1')\n",
    "plt.plot(epochs_range, val_f1, label='Validation F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('capstone-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948fb5d5b69e8e6148da914366d7e8ec0c2c8a66958644710cf8e6ff284ce85b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
