{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My company, <b/>ZeroEyes</b>, provides a service that uses artificial intelligence\n",
    "to identify, prevent & mitigate active shooter situations using pre-existing surveillance\n",
    "cameras. \n",
    "\n",
    "To make every camera frame valuable, I decided to create a tool to help identify when\n",
    "rolling shutter distortion occurs on cameras they monitor to avoid missing a crucial\n",
    "frame when it matters most. For example, if a shooter is caught running down a hallway, swiftly rotating their body, \n",
    "or riding in a fast-moving vehicle, eliminating rolling shutter distortion is paramount.\n",
    "Some potential use cases for the tool I have laid the framework for include using it in the first step in a process to attempt to correct\n",
    "this distortion, or simply use it to identify cameras that often produce images with\n",
    "rolling shutter distortion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "\n",
    "import functions as fn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I highly recommend reviewing the `functions.py` file, which contains essential functions used to preprocess data & apply the artificial rolling shutter effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately, there is no readily available data on rolling shutter distortion.\n",
    "Consequently, I had to synthesize my own by applying a custom rolling shutter effect. Using the COCO 2017 dataset, I was able to create a new dataset using a built-from-scratch\n",
    "rolling shutter effect. \n",
    "\n",
    "Using the COCO 2017 dataset, I was able to create a new dataset using a built-from-scratch\n",
    "rolling shutter effect. Using people as the target for my model, I extracted polygonal\n",
    "segmentation annotations provided by COCO and applied the rolling shutter effect to the area\n",
    "within the segmentaion.\n",
    "\n",
    "One of the advantages of using COCO is that it is a very large dataset, containing over\n",
    "330,000 images in total, 200,000 of which are annotated. For the purposes of my project,\n",
    "since not every image included a person in it, I had about 70,000 images to work with. Another advantage is that it is well-known and oft-used dataset, and is widely-regarded as a \n",
    "benchmark for machine learning algorithms.\n",
    "\n",
    "However, there are some limitations to this dataset. The annotation boundaries are often\n",
    "imprecise. For example, the polygonal segmentation annotations provided typically do not\n",
    "capture all the pixels of any given object, which introduces additional distortion of its \n",
    "own in the data synthesis process. Further, some annotations are outright inaccurate.\n",
    "\n",
    "The following code automatically transforms the images in the `./data/` directory and adds the newly created data into the `./created_data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.56s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataDir='data'\n",
    "trainDir='train2017'\n",
    "valDir='val2017'\n",
    "seed=42\n",
    "transformed=.5\n",
    "train=.8\n",
    "test = 1 - train\n",
    "strategy_list = ['left', 'right']\n",
    "destDataDir='created_data'\n",
    "destTrainDir='train'\n",
    "destTestDir='test'\n",
    "destValDir='val'\n",
    "destNormDir='NORMAL'\n",
    "destRSDir='ROLLING_SHUTTER'\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "trainAnn=f'{dataDir}/annotations/instances_{trainDir}.json'\n",
    "valAnn=f'{dataDir}/annotations/instances_{valDir}.json'\n",
    "\n",
    "cocoTrain=COCO(trainAnn)\n",
    "cocoVal=COCO(valAnn)\n",
    "\n",
    "catIdsTrain = cocoTrain.getCatIds(catNms=['person'])\n",
    "imgIdsTrain = cocoTrain.getImgIds(catIds=catIdsTrain)\n",
    "imgIdsTrain = cocoTrain.getImgIds(imgIds=imgIdsTrain)\n",
    "annIdsTrain = cocoTrain.getAnnIds(imgIds=imgIdsTrain, catIds=catIdsTrain, iscrowd=None)\n",
    "\n",
    "catIdsVal = cocoVal.getCatIds(catNms=['person'])\n",
    "imgIdsVal = cocoVal.getImgIds(catIds=catIdsVal)\n",
    "imgIdsVal = cocoVal.getImgIds(imgIds=imgIdsVal)\n",
    "annIdsVal = cocoVal.getAnnIds(imgIds=imgIdsVal, catIds=catIdsVal, iscrowd=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: depending on your computer, the below code can take anywhere from 30 minutes to a few hours to synthesize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "prog = 0\n",
    "iters = 0\n",
    "limit = 5000 # number of images to process for both RS and normal; results in around double the amount (limit of 200 = ~400 total images)\n",
    "\n",
    "ids_ignore = []\n",
    "\n",
    "# clears destination folders\n",
    "destDir = os.path.join(os.getcwd(), destDataDir.replace('/', '\\\\'))\n",
    "for traintestval in [destTrainDir, destTestDir, destValDir]:\n",
    "    for normrs in [destNormDir, destRSDir]:\n",
    "        files = glob.glob(f'{destDir}\\\\{traintestval}\\\\{normrs}\\\\*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "### ROLLING SHUTTER\n",
    "\n",
    "ids_list = []\n",
    "for i in np.arange(limit):\n",
    "    id = np.random.choice(imgIdsTrain)\n",
    "    if int(id) not in ids_ignore:\n",
    "        ids_list.append(id)\n",
    "\n",
    "for id in ids_list: # rolling shutter\n",
    "    print(f'Loading image id: {id}')\n",
    "\n",
    "    try:\n",
    "        imgDict = cocoTrain.loadImgs([id])[0]\n",
    "        annDict = cocoTrain.loadAnns(cocoTrain.getAnnIds(imgIds=imgDict['id'], catIds=catIdsTrain, iscrowd=None))\n",
    "\n",
    "        fpath = '{}/{}/{}'.format(dataDir, trainDir, imgDict['file_name'])\n",
    "        img = cv2.imread(fpath)\n",
    "        fn.remove_true_blacks(img) # removes true blacks from photo (so data synthesis process is not confused by pre-existing true blacks in images)\n",
    "\n",
    "        ann = annDict[np.random.randint(0, len(annDict))]\n",
    "        strategy = strategy_list[np.random.randint(0, len(strategy_list))] # picks between left or right distortion\n",
    "\n",
    "        masked, maskedInv = fn.generate_masked_images(img, ann) # cuts out annotated section of image, and also returns inverse \"cutout\" (remainder of image after being cut out)\n",
    "\n",
    "        masked_rs = fn.apply_rolling_shutter(masked, ann, intensity=.7, strategy=strategy) # applies rolling shutter effect to masked \"cutout\"\n",
    "\n",
    "        maskedInv_filled = fn.fill_inv_masked(maskedInv, ann, strategy=strategy) # fills missing pixel values in inverse \"cutout\"\n",
    "\n",
    "        final = fn.recombine_masked_imgs(masked_rs, maskedInv_filled) # places post-rolling-shutter-effect \"cutout\" on top of inverse \"cutout\"\n",
    "\n",
    "        if(prog < train): # handles whether or not to save to train or test directory\n",
    "            fpath = '{}/{}/{}/{}'.format(destDataDir, destTrainDir, destRSDir, f'rs-{iters}.png')\n",
    "            cv2.imwrite(fpath, final)\n",
    "        else:\n",
    "            fpath = '{}/{}/{}/{}'.format(destDataDir, destTestDir, destRSDir, f'rs-{iters}.png')\n",
    "            cv2.imwrite(fpath, final)\n",
    "    except:\n",
    "        print('Something went wrong, continuing to next image')\n",
    "\n",
    "    prog += 1 / len(ids_list)\n",
    "    iters += 1\n",
    "    print(f'Finished! {50*prog:.2f}% done')\n",
    "\n",
    "### NORMAL\n",
    "\n",
    "prog = 0\n",
    "iters = 0\n",
    "ids_list = []\n",
    "for i in np.arange(limit):\n",
    "    ids_list.append(np.random.choice(imgIdsTrain))\n",
    "\n",
    "for id in ids_list: \n",
    "    print(f'Loading image id: {id}')\n",
    "\n",
    "    imgDict = cocoTrain.loadImgs([id])[0]\n",
    "    annDict = cocoTrain.loadAnns(cocoTrain.getAnnIds(imgIds=imgDict['id'], catIds=catIdsTrain, iscrowd=None))\n",
    "\n",
    "    fpath = '{}/{}/{}'.format(dataDir, trainDir, imgDict['file_name'])\n",
    "    img = cv2.imread(fpath)\n",
    "    fn.remove_true_blacks(img) # removes true blacks from photo to make sure model doesn't learn that images without true blacks are unmodified\n",
    "\n",
    "    if(prog < train): # handles whether or not to save to train or test directory\n",
    "        fpath = '{}/{}/{}/{}'.format(destDataDir, destTrainDir, destNormDir, f'rs-{iters}.png')\n",
    "        cv2.imwrite(fpath, img)\n",
    "    else:\n",
    "        fpath = '{}/{}/{}/{}'.format(destDataDir, destTestDir, destNormDir, f'rs-{iters}.png')\n",
    "        cv2.imwrite(fpath, img)\n",
    "\n",
    "    prog += 1 / len(ids_list)\n",
    "    iters += 1\n",
    "    print(f'Finished loading! {50 + 50*prog:.2f}% done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, due to time constraints I only had time to train one transfer-learning CNN model. I chose to use MobileNetV3Large for its time efficiency at little cost to performance. I also trained a logistic regression and a random forest model, although they expectedly performed worse than the MobileNetV3Large model did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV3Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7936 files belonging to 2 classes.\n",
      "Using 6349 files for training.\n",
      "Found 7936 files belonging to 2 classes.\n",
      "Using 1587 files for validation.\n",
      "Found 1974 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    'created_data/train', \n",
    "    labels='inferred',\n",
    "    subset=\"training\",\n",
    "    validation_split=.2,\n",
    "    seed=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "val_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    'created_data/train', \n",
    "    labels='inferred',\n",
    "    subset=\"validation\",\n",
    "    validation_split=.2,\n",
    "    seed=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "test_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    'created_data/test', \n",
    "    labels='inferred',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [512,512])\n",
    "    final_image = keras.applications.mobilenet_v3.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess).prefetch(1)\n",
    "val_data = val_data.map(preprocess).prefetch(1)\n",
    "test_data = test_data.map(preprocess).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the below code will take multiple hours to run, even on the highest-end of computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_mobilenetv3 = keras.applications.MobileNetV3Large(weights = 'imagenet', include_top = False)\n",
    "\n",
    "model_dir = 'models'\n",
    "model_uuid = 'model_MobileNetV3_v1'\n",
    "\n",
    "for layer in base_model_mobilenetv3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model_mobilenetv3.output)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(avg)\n",
    "model_mobilenetv3 = keras.Model(inputs = base_model_mobilenetv3.input, outputs = output)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=10, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 0.2, momentum = 0.9, decay = 0.01)\n",
    "model_mobilenetv3.compile(loss = 'binary_crossentropy', optimizer = optimizer,  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "results = model_mobilenetv3.fit_generator(train_data,\n",
    "    epochs=1000,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code has been run once, the following code can be used to obtain the metrics of the best model (and adapted to view those of the other models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "62/62 [==============================] - 56s 872ms/step - loss: 0.7968 - accuracy: 0.5664 - recall_m: 0.2683 - precision_m: 0.4963 - f1_m: 0.3473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.796765148639679,\n",
       " 0.566362738609314,\n",
       " 0.2683192789554596,\n",
       " 0.49627789855003357,\n",
       " 0.34728729724884033]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_mobilenetv3 = keras.applications.MobileNetV3Large(weights = 'imagenet', include_top = False)\n",
    "\n",
    "model_dir = 'models'\n",
    "model_uuid = 'model_MobileNetV3_v1'\n",
    "\n",
    "for layer in base_model_mobilenetv3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model_mobilenetv3.output)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(avg)\n",
    "final_model_mobilenetv3 = keras.Model(inputs = base_model_mobilenetv3.input, outputs = output)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=10, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 0.2, momentum = 0.9, decay = 0.01)\n",
    "final_model_mobilenetv3.compile(loss = 'binary_crossentropy', optimizer = optimizer,  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "final_model_mobilenetv3.load_weights('models/model_MobileNetV3_v1_weights00000059.h5')\n",
    "\n",
    "final_model_mobilenetv3.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression & random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately for these `sklearn` models, I had to convert all the images to `numpy` arrays, meaning I had to store every image in memory. Computers with low available memory may struggle to run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory('../created_data/train', batch_size=8000)\n",
    "test_imgs = keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory('../created_data/test', batch_size=2000)\n",
    "\n",
    "X_i, y_i = next(train_imgs)\n",
    "X_test, y_test = next(test_imgs)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_i, y_i, train_size = 0.75, random_state = seed)\n",
    "\n",
    "X_train = X_train.reshape(5952, -1)\n",
    "X_val = X_val.reshape(1984, -1)\n",
    "X_test = X_test.reshape(1974, -1)\n",
    "\n",
    "y_train = y_train[:,1]\n",
    "y_val = y_val[:,1]\n",
    "y_test = y_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5146316851664985, 0.5207700101317123)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "precision_score(y_test, lr_pred), accuracy_score(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4727694090382387, 0.48226950354609927)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "precision_score(y_test, rf_pred), accuracy_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the complexity of the task set forth, the models performed about as well as I expected. \n",
    "To achieve better results, I would likely need to train on a wider range of data, as well as for a longer period\n",
    "That said, the MobileNetV3Large model managed to eke out a ~10% better accuracy than the \n",
    "simpler models, indicating that the model was able to recognize some elements\n",
    "of the distortion effect, if not in full."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MobileNetV3Large model I have created is a proof of concept that models can be trained to recognize this type of distortion, although is not necessarily ready for implementation. Additionally, some wrinkles in the data synthesis process still must be ironed out; for example, as inaccurate and imprecise COCO annotations\n",
    "hampered the data synthesis process introducing additional, unrelated distortion, more accurately annotated data must be sourced to improve the synthetic data quality. Additionally, pre-existing distortion within the COCO dataset needs to be accounted for.\n",
    "\n",
    "I used accuracy as my primary metric to understand if the model could even recognize rolling shutter distortion, but in future iterations, I would focus more heavily on the precision score of models as rolling shutter is an admittedly rare occurrence and should only be flagged by the model unless it is completely sure it exists in the image.\n",
    "\n",
    "For some future recommendations for continuing this project, I'd like to implement various neural network\n",
    "architectures to achieve better metric scores. Next, I'd want to explore with locating and reversing this distortion in an\n",
    "attempt to repair the image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('capstone-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948fb5d5b69e8e6148da914366d7e8ec0c2c8a66958644710cf8e6ff284ce85b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
