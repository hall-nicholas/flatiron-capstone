{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\anaconda3\\envs\\capstone-env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "\n",
    "import functions as fn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend reviewing the `functions.py` file, which contains essential functions used to preprocess data & apply the artificial rolling shutter effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='data'\n",
    "trainDir='train2017'\n",
    "valDir='val2017'\n",
    "seed=42\n",
    "transformed=.5\n",
    "train=.8\n",
    "test = 1 - train\n",
    "strategy_list = ['left', 'right']\n",
    "destDataDir='created_data'\n",
    "destTrainDir='train'\n",
    "destTestDir='test'\n",
    "destValDir='val'\n",
    "destNormDir='NORMAL'\n",
    "destRSDir='ROLLING_SHUTTER'\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "trainAnn=f'{dataDir}/annotations/instances_{trainDir}.json'\n",
    "valAnn=f'{dataDir}/annotations/instances_{valDir}.json'\n",
    "\n",
    "cocoTrain=COCO(trainAnn)\n",
    "cocoVal=COCO(valAnn)\n",
    "\n",
    "catIdsTrain = cocoTrain.getCatIds(catNms=['person'])\n",
    "imgIdsTrain = cocoTrain.getImgIds(catIds=catIdsTrain)\n",
    "imgIdsTrain = cocoTrain.getImgIds(imgIds=imgIdsTrain)\n",
    "annIdsTrain = cocoTrain.getAnnIds(imgIds=imgIdsTrain, catIds=catIdsTrain, iscrowd=None)\n",
    "\n",
    "catIdsVal = cocoVal.getCatIds(catNms=['person'])\n",
    "imgIdsVal = cocoVal.getImgIds(catIds=catIdsVal)\n",
    "imgIdsVal = cocoVal.getImgIds(imgIds=imgIdsVal)\n",
    "annIdsVal = cocoVal.getAnnIds(imgIds=imgIdsVal, catIds=catIdsVal, iscrowd=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: depending on your computer, the below code can take anywhere from 30 minutes to a few hours to synthesize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "prog = 0\n",
    "iters = 0\n",
    "limit = 5000 # number of images to process for both RS and normal; results in around double the amount (limit of 200 = ~400 total images)\n",
    "\n",
    "ids_ignore = []\n",
    "\n",
    "# clears destination folders\n",
    "destDir = os.path.join(os.getcwd(), destDataDir.replace('/', '\\\\'))\n",
    "for traintestval in [destTrainDir, destTestDir, destValDir]:\n",
    "    for normrs in [destNormDir, destRSDir]:\n",
    "        files = glob.glob(f'{destDir}\\\\{traintestval}\\\\{normrs}\\\\*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "### ROLLING SHUTTER\n",
    "\n",
    "ids_list = []\n",
    "for i in np.arange(limit):\n",
    "    id = np.random.choice(imgIdsTrain)\n",
    "    if int(id) not in ids_ignore:\n",
    "        ids_list.append(id)\n",
    "\n",
    "for id in ids_list: # rolling shutter\n",
    "    print(f'Loading image id: {id}')\n",
    "\n",
    "    try:\n",
    "        imgDict = cocoTrain.loadImgs([id])[0]\n",
    "        annDict = cocoTrain.loadAnns(cocoTrain.getAnnIds(imgIds=imgDict['id'], catIds=catIdsTrain, iscrowd=None))\n",
    "\n",
    "        fpath = '{}/{}/{}'.format(dataDir, trainDir, imgDict['file_name'])\n",
    "        img = cv2.imread(fpath)\n",
    "        fn.remove_true_blacks(img)\n",
    "\n",
    "        ann = annDict[np.random.randint(0, len(annDict))]\n",
    "        strategy = strategy_list[np.random.randint(0, len(strategy_list))]\n",
    "\n",
    "        masked, maskedInv = fn.generate_masked_images(img, ann)\n",
    "\n",
    "        masked_rs = fn.apply_rolling_shutter(masked, ann, intensity=.7, strategy=strategy)\n",
    "\n",
    "        maskedInv_filled = fn.fill_inv_masked(maskedInv, ann, strategy=strategy)\n",
    "\n",
    "        final = fn.recombine_masked_imgs(masked_rs, maskedInv_filled)\n",
    "\n",
    "        if(prog < train):\n",
    "            fpath = '{}/{}/{}/{}'.format(destDataDir, destTrainDir, destRSDir, f'rs-{iters}.png')\n",
    "            cv2.imwrite(fpath, final)\n",
    "        else:\n",
    "            fpath = '{}/{}/{}/{}'.format(destDataDir, destTestDir, destRSDir, f'rs-{iters}.png')\n",
    "            cv2.imwrite(fpath, final)\n",
    "    except:\n",
    "        print('Something went wrong, continuing to next image')\n",
    "\n",
    "    prog += 1 / len(ids_list)\n",
    "    iters += 1\n",
    "    print(f'Finished! {50*prog:.2f}% done')\n",
    "\n",
    "### NORMAL\n",
    "\n",
    "prog = 0\n",
    "iters = 0\n",
    "ids_list = []\n",
    "for i in np.arange(limit):\n",
    "    ids_list.append(np.random.choice(imgIdsTrain))\n",
    "\n",
    "for id in ids_list: \n",
    "    print(f'Loading image id: {id}')\n",
    "\n",
    "    imgDict = cocoTrain.loadImgs([id])[0]\n",
    "    annDict = cocoTrain.loadAnns(cocoTrain.getAnnIds(imgIds=imgDict['id'], catIds=catIdsTrain, iscrowd=None))\n",
    "\n",
    "    fpath = '{}/{}/{}'.format(dataDir, trainDir, imgDict['file_name'])\n",
    "    img = cv2.imread(fpath)\n",
    "    fn.remove_true_blacks(img)\n",
    "\n",
    "    if(prog < train):\n",
    "        fpath = '{}/{}/{}/{}'.format(destDataDir, destTrainDir, destNormDir, f'rs-{iters}.png')\n",
    "        cv2.imwrite(fpath, img)\n",
    "    else:\n",
    "        fpath = '{}/{}/{}/{}'.format(destDataDir, destTestDir, destNormDir, f'rs-{iters}.png')\n",
    "        cv2.imwrite(fpath, img)\n",
    "\n",
    "    prog += 1 / len(ids_list)\n",
    "    iters += 1\n",
    "    print(f'Finished loading! {50 + 50*prog:.2f}% done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV3Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    'created_data/train', \n",
    "    labels='inferred',\n",
    "    subset=\"training\",\n",
    "    validation_split=.2,\n",
    "    seed=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "val_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    'created_data/train', \n",
    "    labels='inferred',\n",
    "    subset=\"validation\",\n",
    "    validation_split=.2,\n",
    "    seed=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "test_data = keras.preprocessing.image_dataset_from_directory(\n",
    "    'created_data/test', \n",
    "    labels='inferred',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [512,512])\n",
    "    final_image = keras.applications.mobilenet_v3.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess).prefetch(1)\n",
    "val_data = val_data.map(preprocess).prefetch(1)\n",
    "test_data = test_data.map(preprocess).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the below code will take multiple hours to run, even on the highest-end of computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_mobilenetv3 = keras.applications.MobileNetV3Large(weights = 'imagenet', include_top = False)\n",
    "\n",
    "model_dir = '../models'\n",
    "model_uuid = 'model_MobileNetV3_v1'\n",
    "\n",
    "for layer in base_model_mobilenetv3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model_mobilenetv3.output)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(avg)\n",
    "model_mobilenetv3 = keras.Model(inputs = base_model_mobilenetv3.input, outputs = output)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=10, min_delta=.00250)\n",
    "model_checkpoint = ModelCheckpoint(f'{model_dir}/{model_uuid}_weights{{epoch:08d}}.h5', verbose = 2, save_best_only=False, period=1)\n",
    "csv_logger = CSVLogger(f'{model_dir}/{model_uuid}.csv', separator = ',', append = True)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 0.2, momentum = 0.9, decay = 0.01)\n",
    "model_mobilenetv3.compile(loss = 'binary_crossentropy', optimizer = optimizer,  metrics = ['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "results = model_mobilenetv3.fit_generator(train_data,\n",
    "    epochs=1000,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[early_stopping, model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression & random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately for these `sklearn` models, I had to convert all the images to `numpy` arrays, meaning I had to store every image in memory. Computers with low available memory may struggle to run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory('../created_data/train', batch_size=8000)\n",
    "test_imgs = keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory('../created_data/test', batch_size=2000)\n",
    "\n",
    "X_i, y_i = next(train_imgs)\n",
    "X_test, y_test = next(test_imgs)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_i, y_i, train_size = 0.75, random_state = seed)\n",
    "\n",
    "X_train = X_train.reshape(5952, -1)\n",
    "X_val = X_val.reshape(1984, -1)\n",
    "X_test = X_test.reshape(1974, -1)\n",
    "\n",
    "y_train = y_train[:,1]\n",
    "y_val = y_val[:,1]\n",
    "y_test = y_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "precision_score(y_test, lr_pred), accuracy_score(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "precision_score(y_test, rf_pred), accuracy_score(y_test, rf_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('capstone-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948fb5d5b69e8e6148da914366d7e8ec0c2c8a66958644710cf8e6ff284ce85b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
